{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318733a0-eb89-4df2-b278-e80afc144cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openicl\n",
    "# Restart the kernel after the installation is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4e6a4-dff9-4f39-a86f-62736ccd82c4",
   "metadata": {},
   "source": [
    "# 2. Using Different Language Models with OpenICL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d14f4-2e73-4b1c-ba9e-0f9112a764b1",
   "metadata": {},
   "source": [
    "In this chapter, we will show you how to use OpenICL to do in-context learning (ICL) with different language models. Mainly including [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [FLAN-T5](https://arxiv.org/abs/2210.11416), [XGLM](https://arxiv.org/abs/2112.10668), OpenAI's [GPT-3](https://arxiv.org/abs/2005.14165) API and [OPT-175B](https://arxiv.org/abs/2205.01068) API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba89e7-2790-4097-9e5d-c75e356e152f",
   "metadata": {},
   "source": [
    "## 2-1 Huggingface Library's Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01fccb-6188-4960-85ee-662af04718ce",
   "metadata": {},
   "source": [
    "In this section, we will take GPT2, FLAN-T5, and XGLM as examples to show you how to use the models in the [huggingface library](https://huggingface.co/models) with OpenICL. Generally speaking, you only need to assign the corresponding name to the `model_name` parameter when declaring `Inferencer`, but we will still provide you with some specific examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa253f5-7d3e-4810-a5fe-ddebc9699551",
   "metadata": {},
   "source": [
    "### 2-1-1 GPT-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a7f087a",
   "metadata": {},
   "source": [
    "This example can be found in [tutorial1](https://github.com/Shark-NLP/OpenICL/blob/main/examples/tutorials/openicl_tutorial1_getting_started.ipynb). But this time, we set `batch_size` for `TopkRetriever` and `PPLInference` to speed up. It can be noticed that the values ​​of the two `batch_size`(s) could be set to be different (`8` and `6`). That is because, at the beginning of retrieval and inference, the corresponding components will receive the complete dataset or the retrieval results for the entire test set, instead of processing the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import DatasetReader, PromptTemplate, TopkRetriever, PPLInferencer\n",
    "\n",
    "# Define a DatasetReader, loading dataset from huggingface.\n",
    "data = DatasetReader('gpt3mix/sst2', input_columns=['text'], output_column='label')\n",
    "\n",
    "# SST-2 Template Example\n",
    "tp_dict = {\n",
    "     0: '</E>Positive Movie Review: </text>',\n",
    "     1: '</E>Negative Movie Review: </text>' \n",
    "}\n",
    "template = PromptTemplate(tp_dict, {'text' : '</text>'}, ice_token='</E>')\n",
    "\n",
    "# TopK Retriever\n",
    "retriever = TopkRetriever(data, ice_num=2, batch_size=8)\n",
    "\n",
    "# Define a Inferencer\n",
    "inferencer = PPLInferencer(model_name='distilgpt2', batch_size=6)\n",
    "\n",
    "# Inference\n",
    "predictions = inferencer.inference(retriever, ice_template=template, output_json_filename='sst2')\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7b09a-2c1a-468e-bf1a-26f69ad51b21",
   "metadata": {},
   "source": [
    "### 2-1-2 XGLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56bb0c2",
   "metadata": {},
   "source": [
    "When it comes to machine translation, it is a good choice to use XGLM. But when using XGLM, we **don't suggest** to set `batch_size` in `GenInferencer`. (When calling the `model.generate` method of [huggingface transformers library](https://huggingface.co/docs/transformers/index), padding is needed if you want to input multiple pieces of data at a time. But we found in the test that if padding exists, the generation of XGLM will be affected). The code is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518e5ee-b58e-4378-a30f-ac0535da5559",
   "metadata": {},
   "source": [
    "### 2-1-3 FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f592cb3-fae4-4295-baa9-1a3998c3ea19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
